# Know-No
LLM's classification ability has been overclaimed as they fail to reveal the LLM's human-level discrimination intelligence truly. To question whether LLMs understand the essence of the classification task or merely select from given options, we propose a new evaluation Benchmark **Know-No** with **OmniAccuracy** to better evaluate LLM performance when gold labels are both present and absent.

This repository contains codes and scripts relevant to the dataset introduced in the paper [LLMs' Classification Performance is Overclaimed](https://arxiv.org/abs/2406.16203).


## Code Structure
 - `src/`: contains the scripts used for the experiments.
 - `data/`: contains the data used in the Know-No benchmark.
 - `results/`: contains the raw results obtained from selected LLMs.

## Citation 
Please cite the following work if you want to refer to this work: 
```
@misc{xu2024llms,
      title={LLMs' Classification Performance is Overclaimed}, 
      author={Hanzi Xu and Renze Lou and Jiangshu Du and Vahid Mahzoon and Elmira Talebianaraki and Zhuoan Zhou and Elizabeth Garrison and Slobodan Vucetic and Wenpeng Yin},
      year={2024},
      eprint={2406.16203},
      archivePrefix={arXiv},
}
```

## Contact
Hanzi Xu(hanzi.xu@temple.edu)


