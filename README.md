# Know-No
We propose a new evaluation Benchmark Know-No with OmniAccuracy to better evaluate LLM performance when gold labels are both present and absent.

This repository contains codes and scripts relevant to the dataset introduced in this [paper](https://arxiv.org/abs/2406.16203).


## Code Structure
 - `src/`: contains the scripts used for the experiments.
 - `data/`: contains the data used in the Know-No benchmark.
 - `results/`: contains the raw results obtained from selected LLMs.

## Citation 
Please cite the following work if you want to refer to this work: 
```
@misc{xu2024llms,
      title={LLMs' Classification Performance is Overclaimed}, 
      author={Hanzi Xu and Renze Lou and Jiangshu Du and Vahid Mahzoon and Elmira Talebianaraki and Zhuoan Zhou and Elizabeth Garrison and Slobodan Vucetic and Wenpeng Yin},
      year={2024},
      eprint={2406.16203},
      archivePrefix={arXiv},
}
```

## Contact
Hanzi Xu(hanzi.xu@temple.edu)


